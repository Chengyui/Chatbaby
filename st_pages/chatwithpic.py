import streamlit as st
import base64
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain_community.llms import Ollama
from schema.streamhandler import StreamHandler
from schema.ollama_models_db import get_ollama_models
from typing import Any, List, Optional
import requests
import json
from langchain_core.outputs import Generation, LLMResult
from langchain.callbacks.manager import CallbackManagerForLLMRun

def query_ollama(image_base64, prompt="è¯·ç›´æ¥è¿”å›å›¾ä¸­æ–‡å­—ä¿¡æ¯ï¼Œè‹¥æ²¡æœ‰æ–‡å­—ä¿¡æ¯åˆ™è¯¦ç»†æè¿°å›¾ä¸­ç‰©ä½“"):
    url = "http://localhost:11434/v1/chat/completions"
    headers = {
        "Content-Type": "application/json"
    }
    data = {
        "model": "minicpm-v:latest",
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": f"data:image/png;base64,{image_base64}"
                    }
                ]
            }
        ],
        "stream": False,
        "keep_alive":100
    }

    try:
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()


        content = response.content

        encodings = ['utf-8', 'ascii', 'latin1']
        for encoding in encodings:
            try:
                decoded_content = content.decode(encoding)
                print(json.loads(decoded_content)["choices"][0]["message"]["content"])
                return json.loads(decoded_content)
            except UnicodeDecodeError:
                continue
            except json.JSONDecodeError:
                continue


        st.error("Failed to decode response. Raw response:")
        st.text(content)
        return None
    except requests.exceptions.RequestException as e:
        st.error(f"Error communicating with Ollama: {e}")
        return None


def get_conversation_chain(model_name: str) -> ConversationChain:
    """åˆå§‹åŒ–æ”¯æŒå›¾ç‰‡çš„å¯¹è¯é“¾"""
    llm = Ollama(
        model=model_name,
        temperature=0.2,
        base_url="http://localhost:11434",
        keep_alive=3600,
    )

    prompt = PromptTemplate(
        input_variables=["history", "input"],
        template="""Current conversation:
                    {history}
                    Human: {input}
                    Assistant:""")

    memory = ConversationBufferMemory(return_messages=True)
    return ConversationChain(llm=llm, memory=memory, prompt=prompt, verbose=True)


# def on_model_change():
#     """æ¨¡å‹åˆ‡æ¢å›è°ƒå‡½æ•°"""
#     st.session_state.messages = []
#     st.session_state.conversation = None
#     st.session_state.uploaded_image = None

# ä¿®æ”¹æ¨¡å‹åˆ‡æ¢å›è°ƒï¼ˆä¿ç•™å›¾ç‰‡ä¸Šä¸‹æ–‡ï¼‰
def on_model_change():
    # st.session_state.messages = []
    # st.session_state.conversation = None
    # st.session_state.uploaded_image = None
    # æ³¨æ„ï¼šä¸å†æ¸…é™¤ uploaded_image å’Œ image_description
    # st.session_state.messages = []
    # st.session_state.conversation = None
    # st.session_state.uploaded_image = None
    # st.session_state.image_description = None
    st.session_state.image_prompt_added = False
    pass
def run():
    """ä¸»è¿è¡Œå‡½æ•°"""
    st.markdown('''
    <style>
        .uploaded-image {max-width: 300px; margin-top: 10px;}
    </style>
    ''', unsafe_allow_html=True)

    st.markdown('''
    <div class="header-container">
        <p class="header-subtitle">ğŸ¤– æ”¯æŒå¤šæ¨¡æ€çš„æ™ºèƒ½èŠå¤©åŠ©æ‰‹</p>
    </div>
    </div>
    ''', unsafe_allow_html=True)

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    if 'conversation' not in st.session_state:
        st.session_state.conversation = None
    if 'uploaded_image' not in st.session_state:
        st.session_state.uploaded_image = None

    # è·å–å¯ç”¨æ¨¡å‹
    models = get_ollama_models()
    if not models:
        st.warning("è¯·å…ˆå¯åŠ¨OllamaæœåŠ¡")
        return

    # æ¨¡å‹é€‰æ‹©
    st.subheader("é€‰æ‹©è¯­è¨€æ¨¡å‹:")
    col1, _ = st.columns([2, 6])
    with col1:
        model_name = st.selectbox(
            "æ¨¡å‹",
            models,
            format_func=lambda x: f'ğŸ”® {x}',
            key="model_select",
            on_change=on_model_change,
            label_visibility="collapsed"
        )

    # Initialize conversation if needed
    if st.session_state.conversation is None:
        st.session_state.conversation = get_conversation_chain(model_name)

    # Display chat history
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            try:
                img = message["image"]
                st.image(f"data:image/png;base64,{img}",
                         use_column_width=True,
                         caption="ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡")
                st.markdown(message["content"])
            except:
                st.markdown(message["content"])


    # å›¾ç‰‡ä¸Šä¼ ç»„ä»¶
    # if "image_description" not in st.session_state:
    uploaded_file = st.file_uploader(
        "ä¸Šä¼ å›¾ç‰‡ï¼ˆæ”¯æŒPNG/JPGï¼‰",
        type=["png", "jpg", "jpeg"],
        key="image_uploader",
        label_visibility="collapsed"
    )

    # å¤„ç†ä¸Šä¼ çš„å›¾ç‰‡ï¼ˆä¿®æ”¹è¿™éƒ¨åˆ†ï¼‰
    if uploaded_file is not None and "image_description" not in st.session_state:
        bytes_data = uploaded_file.getvalue()
        base64_image = base64.b64encode(bytes_data).decode('utf-8')
        st.session_state.uploaded_image = base64_image

        # å®æ—¶æ˜¾ç¤ºé¢„è§ˆ
        st.session_state.messages.append({
            "role": "user",
            "image": base64_image,
            "content":""
        })
        # st.image(bytes_data, caption="å·²ä¸Šä¼ å›¾ç‰‡")

        # è‡ªåŠ¨è§¦å‘å›¾ç‰‡åˆ†æï¼ˆæ–°å¢éƒ¨åˆ†ï¼‰
        # if "minicpm" in model_name.lower():
        with st.spinner('æ­£åœ¨åˆ†æå›¾ç‰‡...'):
            analysis_result = query_ollama(base64_image)
            if analysis_result:
                st.session_state.image_description= analysis_result["choices"][0]["message"]["content"]
                # æ˜¾ç¤ºå¹¶ä¿å­˜ç»“æœ
                with st.chat_message("assistant"):
                    response_placeholder = st.empty()
                    try:
                        st.image(f"data:image/png;base64,{base64_image}",
                                 use_column_width=True,
                                )
                        # æ˜¾ç¤ºå¹¶ä¿å­˜ç»“æœ
                        response_placeholder.markdown("è§†è§‰åˆ†æç»“æœï¼š"+st.session_state.image_description)
                        st.session_state.messages.append({
                            "role": "assistant",
                            "image": base64_image,
                            "content": f"å›¾ç‰‡åˆ†æå®Œæˆï¼š{st.session_state.image_description}",
                        })
                    except Exception as e:
                        error_msg = f"ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {str(e)}"
                        response_placeholder.error(error_msg)
                        st.session_state.messages.append({"role": "assistant", "content": error_msg})

    # å¤„ç†ç”¨æˆ·è¾“å…¥ï¼ˆä¿®æ”¹è¾“å…¥å¤„ç†é€»è¾‘ï¼‰
    if prompt := st.chat_input(f"ä¸{model_name}å¯¹è¯..."):
        full_prompt = prompt

        # å¦‚æœæœ‰å›¾ç‰‡æè¿°åˆ™è‡ªåŠ¨é™„åŠ åˆ°è¾“å…¥ï¼ˆæ–°å¢ä¸Šä¸‹æ–‡æ‹¼æ¥ï¼‰
        if 'image_description' in st.session_state and st.session_state.image_prompt_added is False:
            full_prompt = f"[å›¾ç‰‡å†…å®¹ï¼š{st.session_state.image_description}] {prompt}"
            st.session_state.image_prompt_added = True

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
        user_message = {"role": "user", "content": full_prompt}
        # if st.session_state.get('uploaded_image'):
        #     user_message["image"] = st.session_state.uploaded_image
        st.session_state.messages.append(user_message)

        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯ï¼ˆå¢åŠ æè¿°æ˜¾ç¤ºï¼‰
        with st.chat_message("user"):
            st.markdown(prompt)  # ä»…æ˜¾ç¤ºåŸå§‹è¾“å…¥
            # if st.session_state.get('uploaded_image'):
            #     st.image(f"data:image/png;base64,{st.session_state.uploaded_image}",
            #              use_column_width=True,
            #              caption="")
            #     # æ˜¾ç¤ºåˆ†æç»“æœæç¤ºï¼ˆæ–°å¢ï¼‰
            #     if 'image_description' in st.session_state:
            #         st.caption(f"å½“å‰å¯¹è¯åŒ…å«å›¾ç‰‡ä¸Šä¸‹æ–‡ï¼š{st.session_state.image_description}")

        # ç”Ÿæˆå›å¤ï¼ˆä¼˜åŒ–æ¨¡å‹åˆ‡æ¢æ”¯æŒï¼‰
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            try:
                # # è§†è§‰æ¨¡å‹ç‰¹æ®Šå¤„ç†ï¼ˆæ–°å¢åˆ¤æ–­é€»è¾‘ï¼‰
                # if "minicpm" in model_name.lower():
                #     # ç›´æ¥ä½¿ç”¨å›¾ç‰‡åˆ†æç»“æœ
                #     response = st.session_state.image_description
                # else:
                # æ™®é€šå¯¹è¯æµç¨‹ï¼ˆä¿®æ”¹ä¸ºä½¿ç”¨full_promptï¼‰
                stream_handler = StreamHandler(response_placeholder)
                st.session_state.conversation.llm.callbacks = [stream_handler]

                response = st.session_state.conversation.run(
                    input=full_prompt  # ä½¿ç”¨åŒ…å«ä¸Šä¸‹æ–‡çš„å®Œæ•´prompt
                )

                # æ˜¾ç¤ºå¹¶ä¿å­˜ç»“æœ
                response_placeholder.markdown(response)
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": response
                })

            except Exception as e:
                error_msg = f"ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {str(e)}"
                response_placeholder.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})
